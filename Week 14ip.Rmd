---
title: "Week 14 IP"
author: "Jenipher Mawia"
date: "11/13/2020"
output:
  html_document: default
  word_document: default
---

# 1. Problem Definition

## 1.1 Defining the question
- Perform dimensionality reduction on the data provided 
- Also perform feature selection on the same data

## 1.2 Specifying the question
Implement the solutions using unsupervised learning techniques for :

- dimensionality reduction: reduce your dataset to a low dimensional dataset using the t-SNE algorithm or PCA 

- and feature selection

# 2. Defining the metrics for success
This project will be considered a success if the following are achieved:
- Unsupervised learning techniques are used for dimensionality reduction and feature selection without any errors.

# 3. The Context
You are a Data analyst at Carrefour Kenya and are currently undertaking a project that will inform the marketing department on the most relevant marketing strategies that will result in the highest no. of sales (total price including tax). Your project has been divided into two parts where you'll explore a recent marketing dataset by performing various unsupervised learning techniques and later providing recommendations based on your insights.

# 4. Experimental Design taken
The project consists of two parts. The following is the order in which I went about to achieve the objectives of this project: 

- Data Sourcing and Understanding
- Checking the data (head and tail, shape(number of records), datatypes)
- Data cleaning procedures (handling null values,outliers, anomalies)
- Exploratory data analysis (Univariate, Bivariate and Multivariate analyses)
- Implementing the solution(Clustering)
- Conclusion and recommendation


# 5. Data Sourcing
The data used for this project was provided by Moringa School and can be downloaded [here](http://bit.ly/CarreFourDataset)

### Reading the data

```{r}
superdata <- read.csv("http://bit.ly/CarreFourDataset")
```

# 6. Checking the Data
### checking the top of the data

```{r}
# checking the first 6 rows in the data
head(superdata)
```

### checking the bottom of the data

```{r}
# checking the last 6 rows in the data
tail(superdata)
```

### checking the shape of the data

```{r}
# checking the dimensions of the data (number of entries and fields)
dim(superdata)
```

The data has 1000 entries and 16 columns. 

### checking the datatypes of the column

```{r}
# getting the datatypes of each column
str(superdata)
```

The data consists of columns that contain numeric, integer and character datatypes. 

## Checking the number of unique values in each column

```{r}
lengths(lapply(superdata, unique))
```

There exists categorical datatypes in the data as shown by the number of unique values per column.


# 7. Appropriateness of the available data to answer the given question

The data contains columns such as: 

- Invoice id which has the invoice number of a given transaction made by a customer. It should be unique for each customer.

- Branch: This suggests that there are more than one branches of the same store from which customers in different regions can shop from. 

- Customer type: the type of customer either a member or normal customer.

- Gender: sex of the customer

- Product line: defining the category from which a customer purchased a product from

- Unit price: definng the price of the product per unit

- Quantity: number of products bought

- Tax: amount of tax charged

- Date of transaction

- Time of the day the purchase was made

- Payment: type of payment method used: by cash, credit card, ewallet etc. 

- Cogs: Cost of goods sold

-  Gross Margin Percentage: percentage change in the gross margin when the purchase is made

- Gross Income: gross income generated from the purchase

- Rating: rating of the transaction in form of numeric values, low values could indicate a poor rating while high values suggest good rating

- Total: total amount generated from the purchase


All these fields are useful in informing the marketing department on the most relevant marketing strategies that will result in the highest number of sales

Therefore, it can be concluded that the data available is appropriate and relevant to answer the given question.

# 8. Data Cleaning

## 8.1 Standardizing column names

Column names should be in the same format to ensure consistency

```{r}
# change column names to all lowercase
colnames(superdata) = tolower(colnames(superdata))

# check changes made
colnames(superdata)

# change two and three letter columns to a standard format
names(superdata)[names(superdata) == "invoice.id"] <- "invoice_id"
names(superdata)[names(superdata) == "customer.type"] <- "customer_type"
names(superdata)[names(superdata) == "product.line"] <- "product_line"
names(superdata)[names(superdata) == "unit.price"] <- "unit_price"
names(superdata)[names(superdata) == "gross.margin.percentage"] <- "gross_margin_percentage"
names(superdata)[names(superdata) == "gross.income"] <- "gross_income"

#check changes made
colnames(superdata)
```

## 8.2 Datatype conversion

Some columns such as rating, customer type, branch, gender, payment and product line are categorical fields but are in numeric or character data types. These need to be converted to factors. 

```{r}
# convert the datatypes to factors
superdata$rating <-as.factor(superdata$rating)
superdata$customer_type <-as.factor(superdata$customer_type)
superdata$branch <-as.factor(superdata$branch)
superdata$product_line <-as.factor(superdata$product_line)
superdata$gender <-as.factor(superdata$gender)
superdata$payment <-as.factor(superdata$payment)

# check the datatypes once more to see changes made
str(superdata)
```

## 8.3 Duplicated Entries

```{r}
#Checking for duplicated rows
duplicates <- superdata[duplicated(superdata),]
dim(duplicates)
```

There are no duplicated entries in the data.

## 8.4 Missing Values

```{r}
# check for missing values in each column in the data
colSums(is.na(superdata))
```

There are no missing values in the data

## 8.5 Outliers

```{r}
# get numerical columns from the data
nums <- unlist(lapply(superdata, is.numeric))

# output the numeric columns in form of a dataframe and check the top of the  resulting dataframe
numeric_cols <- superdata[ , nums]
head(numeric_cols)
```

There are 7 numeric columns from the total 16. 

```{r}
#boxplot(numeric_cols)

# make a plot of multiple boxplots to check for outliers
par ( mfrow= c (  2, 4 ))
for (i in 1 : length (numeric_cols)) {
boxplot (numeric_cols[,i], main= names (numeric_cols[i]), type= "l" )
}
```

There are a few outliers present in the data. We will not remove them because of the dynamics that usually occur in purchases where some customers can be extremely extravagant and others extremely conservative hence causing outliers in the data. Removing them will make the resulting data not be a picture of the actual data. 

# 9. Exploratory Data Analysis

## 9.1 Univariate Data Analysis

### Measures of central tendency

#### Mean

```{r}
# get the mean of all numerical columns
library(ggplot2)
library(psych)

colMeans(numeric_cols)
```

#### Median

```{r}
# Get the median of all numerical columns
apply (numeric_cols, 2 ,median)
```

#### Mode

```{r}
# Create the function.
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}

lapply(superdata,FUN=getmode)
```

- Branch A is the most popular branch of the three branches. 

- Most customers visiting the store are members and most of them are female.

- The most popular product line is the Fashion accessories.

- Most customers made payment through Ewallet

- Most customers gave a rating or 6

- The most popular time is 1948 hours

### Measures of dispersion

#### Find the minimum, maximum and quantiles of the columns in the data.

```{r}
summary(numeric_cols)
```

#### Range

Range is the difference between the maximum point and the minimum point in a set of data. 

```{r}
# get the range for all numeric columns
lapply(numeric_cols,FUN=range)
```
#### Interquartile Range
The interquartile range also commonly known as IQR is the range between the 1st and 3rd
quantiles. It is the difference between the two quantiles.

```{r}
# get the IQR for the numeric columns
lapply(numeric_cols,FUN=IQR)
```

#### Standard Deviation
Find the standard deviation of the numerical columns in the data
```{r}
apply (numeric_cols, 2 ,sd)
```
#### Variance
Find the variance of the numerical columns
```{r}
sapply (numeric_cols, var)
```

The variables appear to be measured in different units hence contributing to the fact that some variables have larger variances than others. 

#### Histograms
```{r}
par( mfrow= c ( 2 , 4 ))
for(i in 1 : length(numeric_cols)) {
hist(numeric_cols[,i], main= names(numeric_cols[i]))
}
```

The columns: tax, cogs, gross income, total are skewed to the right. 

```{r}
str(superdata)
```

## 9.2 Bivariate and Multivariate analysis

### Correlation between the different variables

```{r}
# Checking the correlation coefficients for numeric variables
library(ggcorrplot)
library(corrplot)

correlations <- round(cor(numeric_cols[-5]), 2 )

corrplot(correlations, method = "color", type = "full", tl.col = "black", tl.srt = 45, addCoef.col = "black")

#ggcorrplot(corr, ggtheme = ggplot2::theme_gray, colors = c("#6D9EC1", "white", "#E46726"), lab = T)
```

There is a high correlation between the numeric variables except the quantity column. This is quite expected as they relate to a specific purchase made by a customer i.e. the cost of goods sold will be dependent on tax, gross income is a function of cost of goods sold and net tax, total income is a function of net pay given by gross income minus tax charged. These relationships and co-dependence result in high correlations between the variables

### Scatter Plot
```{r}
plot(superdata$gross_income, superdata$total, xlab="Gross Income", ylab="Total Income")
```

As expected, the scatter plot distribution between these two values follows a straight line. This shows a linear relationship as shown above from the correlation plot. 

Since all numerical columns are highly related, it is expected that they will take the same shape of distribution when plotted. 


# 10. Implementing the Solution

## 10.1 Data Pre-processing
 
Before we begin modelling, we must ensure that the datatypes in the data we will use are in the appropriate mode i.e. numeric. 

```{r}
# save data to a new dataframe to avoid messing up with original data
data <- superdata

# change datatypes
data$branch <- as.numeric(data$branch)
data$customer_type <- as.numeric(data$customer_type)
data$gender <- as.numeric(data$gender)
data$product_line <- as.numeric(data$product_line)
data$payment <- as.numeric(data$payment)
data$rating <- as.numeric(data$rating)

#check the datatypes
str(data)
```

Since we will be implementing unsupervised learning algorithms, it is also important to remove the target variable "Total". We will also exclude columns that are not numerical. 

```{r}
# remove the target and character variables
df <- data[c(-16, -1,-9, -10, -13)]
str(df)
```

## 10.2 Dimensionality Reduction

Dimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data.

There are various ways of dimensionality reduction. We will apply PCA for this project. 

PCA uses linear combinations of the variables, known as principal components. The new projected variables (principal components) are uncorrelated with each other and are ordered so that the first few components retain most of the variation present in the original variables. Thus, PCA is useful in situations where the independent variables are correlated with each other. We have observed this aspect of multicolinearity from the correlation matrix above.  

```{r}
# apply pca on the data
df_pca <- prcomp(df, center = TRUE, scale. = TRUE)

# preview the summary of the pca object
summary(df_pca)
```

As a result, we obtain 11 principal components. 

The first principal component explains a huge percentage of the variance at 35.77%. PC2 explains 10%, PC3 and PC4 explain 9% of the variance and PC5 and PC6 explain 8% of the variance. 

We can conclude that the first 5 components explain 73.95% of the variance, hence we can reduce the dimensions of the original data to 5 components

```{r}
# Calling str() to have a look at the PCA object
str(df_pca)
```

Here we note on the pca object: 

- The center point ($center), 

- scaling ($scale),

- standard deviation(sdev) of each principal component,

- the relationship (correlation or anticorrelation, etc) between the initial variables and the principal components ($rotation), and

-the values of each sample in terms of the principal components ($x)


#### Plotting the PCA object

```{r}
# plotting the first 2 principal components
library(devtools)
library(ggbiplot)
ggbiplot(df_pca)
```

From the graph we can see that the variables quantity, unit price, gross income and tax contibute to PC1 with higher values in those variables moving the samples to the right on the plot, while variables such as product line and payment contribute to PC2.

The first 2 principal components explain 46% of the variance, which is almost half of the total variance.

```{r}
#plotting the third and fourth components
ggbiplot(df_pca,ellipse=TRUE,choices=c(3,4))

```

PC3 and PC4 explain very small percentages of the total variation hence it is difficult to derive insights from the plot.  



```{r}

str(superdata)
```

Adding more detail to the plot of PC1 and PC2

```{r}
library(ggfortify)
pca.plot <- autoplot(df_pca, data = superdata, colour="gender")
pca.plot
```

More females appear to be concentrated below in the graph than males who appear to be above in the graph implying that gender contibutes to PC2.

### PCA dimension reduction with 5 components

Since we concluded earlier that 5 components explain a higher percentage of variance, we will go ahead and create a pca component of only 5 components

```{r}
library(FactoMineR)

# apply PCA
df_PCA = PCA(df, scale.unit = TRUE, ncp = 5, graph = TRUE)
```

From the graph of components, we can still observe the same variables we listed above to be contributing to PC1 and PC2

#### Interpreting Principal Components

```{r}
#check the correlation of the features to the principal components
df_PCA$var$coord
```

From the outputs, we can observe that: 

- unit price, quantity, tax, cogs and gross income are highly correlated with PC1.
This correlation suggests the five variables vary together and when one goes down, the others decrease as well

- branch, gender, payment are highly correlated with PC2

- customer type, product line are highly correlated with PC3

- rating is highly correlated with PC5

## 10.3 Feature Selection 























































































































